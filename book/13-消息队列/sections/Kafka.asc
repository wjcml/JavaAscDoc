=== Kafka

Kafka java语言开发，吞吐量大。

==== Kafka的组件

*生产者（Producer），消费者（Consumer），经纪人（Broker），主题（Topic），分区（Queue）*


==== Kafka的模型

*发布-订阅模型*

==== Kafka多副本机制

 Kafka 为分区（Partition）引入了多副本（Replica）机制。分区（Partition）中的多个副本之间会有一个叫做 leader 的家伙，其他副本称为 follower。我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。


==== Zookeeper 在 Kafka 中的作用

. *Broker注册*： Zookeeper 上会有一个专门 *用来进行 Broker 服务器列表记录的节点*。
. *负载均衡*


==== Kafka如何保证消息的消费顺序

第一种方法： *Kafka 可以保证 Partition 中的消息有序，所以只要保证一个 Topic 只有一个 分区就能够保证消息的消费顺序*。

第二种方法： Kafka 中发送 1 条消息的时候，可以指定 topic, partition, key,data（数据） 4 个参数，*发送消息的时候指定分区发送也可以保证消息的消费顺序*。


==== Kafka如何保证消息不丢失

Kafka 消息丢失分三种情况，生产者丢失消息，kafka自己丢失消息，消费者丢失消息。

下面根据这三种情况分别提出解决方案：

===== 生产者丢失消息

*使用带回调方法的 API*。如下代码 API：

[source, java]
----
   Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback)
----

使用带有回调方法的 API 时，我们可以根据回调函数得知消息是否发送成功，如果发送失败了，我们要进行异常处理，比如把失败消息存储到本地硬盘或远程数据库，等应用正常了再发送，这样才能保证消息不丢失。

这里我们还需要设置 *重试次数（retries）*，和 *重试时间间隔（retry.backoff.ms=300）*


===== kafka自己丢失消息

设置参数 *保证 leader 副本能有 follower 副本*，并 *保证至少需要两个副本被写入数据才算成功*。


===== 消费者丢失消息

关闭 *自动提交 offset*，每次在真正消费完消息之后之后再自己 *手动提交 offset*。


==== Kafka如何保证消息不重复

===== 生产端不重复生产消息

每个生产端生成一个唯一的 ID，并且在每条消息中生成一个 *sequence num*，sequence num 是 *递增且唯一* 的，这样就能对消息去重，达到一个生产端不重复发送一条消息的目的。

===== 消费端不能重复消费消息

通过对每条消息设置一个 ID，每次消费前，校验一下此条消息是否已经被消费过了。


==== Kafka常见问题

===== Kafka 重启消费者后，从分区的哪里开始重新消费（队列头？队列尾？上一次消费位置？）

参考文档： https://blog.csdn.net/lishuangzhe7047/article/details/74530417["Kafka auto.offset.reset值详解", window="_blank"]

这个问题的关键在于 *auto.offset.reset* 的设置，*auto.offset.reset* 有三个值，*earliest，latest，none*。

*earliest*： 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费。 + 
*latest*： 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，只消费当前消费者启动完成后生产者新生产的数据。 + 
*none*： topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常








